{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7db876-160c-4348-a0d8-69961f4f2459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Chest pain type</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FBS over 120</th>\n",
       "      <th>EKG results</th>\n",
       "      <th>Max HR</th>\n",
       "      <th>Exercise angina</th>\n",
       "      <th>ST depression</th>\n",
       "      <th>Slope of ST</th>\n",
       "      <th>Number of vessels fluro</th>\n",
       "      <th>Thallium</th>\n",
       "      <th>Heart Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Presence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
       "0   0   58    1                4  152          239             0            0   \n",
       "1   1   52    1                1  125          325             0            2   \n",
       "2   2   56    0                2  160          188             0            2   \n",
       "\n",
       "   Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
       "0     158                1            3.6            2   \n",
       "1     171                0            0.0            1   \n",
       "2     151                0            0.0            1   \n",
       "\n",
       "   Number of vessels fluro  Thallium Heart Disease  \n",
       "0                        2         7      Presence  \n",
       "1                        0         3       Absence  \n",
       "2                        0         3       Absence  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./train.csv')\n",
    "train_df = pd.DataFrame(train, columns = train.columns)#DataFrame\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5455b9-bfc5-4998-bb3c-0417cc9c7373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [1 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_df.iloc[:,:-1]\n",
    "train_target = train_df.iloc[:,-1]\n",
    "\n",
    "#target 데이터 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "target_encoded = le.fit_transform(train_target)\n",
    "print(type(target_encoded),target_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1242140e-af4d-49a0-87c8-3d8090cf95ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Chest pain type</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FBS over 120</th>\n",
       "      <th>EKG results</th>\n",
       "      <th>Max HR</th>\n",
       "      <th>Exercise angina</th>\n",
       "      <th>ST depression</th>\n",
       "      <th>Slope of ST</th>\n",
       "      <th>Number of vessels fluro</th>\n",
       "      <th>Thallium</th>\n",
       "      <th>Heart Disease</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
       "0   0   58    1                4  152          239             0            0   \n",
       "1   1   52    1                1  125          325             0            2   \n",
       "2   2   56    0                2  160          188             0            2   \n",
       "\n",
       "   Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
       "0     158                1            3.6            2   \n",
       "1     171                0            0.0            1   \n",
       "2     151                0            0.0            1   \n",
       "\n",
       "   Number of vessels fluro  Thallium  Heart Disease  target  \n",
       "0                        2         7            NaN       1  \n",
       "1                        0         3            NaN       0  \n",
       "2                        0         3            NaN       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded = pd.DataFrame(train_data, columns = train.columns)\n",
    "train_encoded['target'] = target_encoded\n",
    "train_encoded.head(3)#absenece가 0, presence가 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "051bab6a-9ce5-4ff3-8590-6d4badd38a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.drop('Heart Disease', axis = 1, inplace = True)#axis = 1은 필수\n",
    "train_encoded.drop('id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f85a5fb-c915-4476-8fce-af5bddd46a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Chest pain type</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FBS over 120</th>\n",
       "      <th>EKG results</th>\n",
       "      <th>Max HR</th>\n",
       "      <th>Exercise angina</th>\n",
       "      <th>ST depression</th>\n",
       "      <th>Slope of ST</th>\n",
       "      <th>Number of vessels fluro</th>\n",
       "      <th>Thallium</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629995</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629996</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629997</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629998</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629999</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age    Sex  Chest pain type     BP  Cholesterol  FBS over 120  \\\n",
       "0       False  False            False  False        False         False   \n",
       "1       False  False            False  False        False         False   \n",
       "2       False  False            False  False        False         False   \n",
       "3       False  False            False  False        False         False   \n",
       "4       False  False            False  False        False         False   \n",
       "...       ...    ...              ...    ...          ...           ...   \n",
       "629995  False  False            False  False        False         False   \n",
       "629996  False  False            False  False        False         False   \n",
       "629997  False  False            False  False        False         False   \n",
       "629998  False  False            False  False        False         False   \n",
       "629999  False  False            False  False        False         False   \n",
       "\n",
       "        EKG results  Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
       "0             False   False            False          False        False   \n",
       "1             False   False            False          False        False   \n",
       "2             False   False            False          False        False   \n",
       "3             False   False            False          False        False   \n",
       "4             False   False            False          False        False   \n",
       "...             ...     ...              ...            ...          ...   \n",
       "629995        False   False            False          False        False   \n",
       "629996        False   False            False          False        False   \n",
       "629997        False   False            False          False        False   \n",
       "629998        False   False            False          False        False   \n",
       "629999        False   False            False          False        False   \n",
       "\n",
       "        Number of vessels fluro  Thallium  target  \n",
       "0                         False     False   False  \n",
       "1                         False     False   False  \n",
       "2                         False     False   False  \n",
       "3                         False     False   False  \n",
       "4                         False     False   False  \n",
       "...                         ...       ...     ...  \n",
       "629995                    False     False   False  \n",
       "629996                    False     False   False  \n",
       "629997                    False     False   False  \n",
       "629998                    False     False   False  \n",
       "629999                    False     False   False  \n",
       "\n",
       "[630000 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 전처리\n",
    "train_encoded.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa45cd5-acdc-4805-a8cb-83011817b61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 630000 entries, 0 to 629999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Age                      630000 non-null  int64  \n",
      " 1   Sex                      630000 non-null  int64  \n",
      " 2   Chest pain type          630000 non-null  int64  \n",
      " 3   BP                       630000 non-null  int64  \n",
      " 4   Cholesterol              630000 non-null  int64  \n",
      " 5   FBS over 120             630000 non-null  int64  \n",
      " 6   EKG results              630000 non-null  int64  \n",
      " 7   Max HR                   630000 non-null  int64  \n",
      " 8   Exercise angina          630000 non-null  int64  \n",
      " 9   ST depression            630000 non-null  float64\n",
      " 10  Slope of ST              630000 non-null  int64  \n",
      " 11  Number of vessels fluro  630000 non-null  int64  \n",
      " 12  Thallium                 630000 non-null  int64  \n",
      " 13  target                   630000 non-null  int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 67.3 MB\n"
     ]
    }
   ],
   "source": [
    "train_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d3faa5c-d3fc-4890-aa94-22ce414d557e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                        0\n",
      "Sex                        0\n",
      "Chest pain type            0\n",
      "BP                         0\n",
      "Cholesterol                0\n",
      "FBS over 120               0\n",
      "EKG results                0\n",
      "Max HR                     0\n",
      "Exercise angina            0\n",
      "ST depression              0\n",
      "Slope of ST                0\n",
      "Number of vessels fluro    0\n",
      "Thallium                   0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_encoded.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329fef08-c023-465a-a634-568c0b274e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, target_encoded, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f73c3b96-777a-495b-801c-1ea239745ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862484126984127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "dt = DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 4)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_predict = dt.predict(X_test)\n",
    "print(accuracy_score(y_test,dt_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbd1d5a4-0fee-4d53-b302-aaf7013817dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8626349206349205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "score = cross_val_score(dt, train_data, target_encoded, cv = 5)#전체 x, 전체 y를 집어넣어야 함\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42539aea-05fe-485f-80a1-7d8e1252e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 9, 'min_samples_leaf': 5}\n",
      "0.8801587301587301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pred = grid_dt.predict(X_test) 이것만으로도 가능(간단)\\nbest_model = grid_dt.best_estimator_\\npred = best_model.predict(X_test)이 원래는 정석'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth' : [3,5,7,9],\n",
    "         'min_samples_leaf': [2,3,4,5]}\n",
    "grid_dt = GridSearchCV(dt, param_grid = params, cv = 5)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "print(grid_dt.best_params_)\n",
    "best_model = grid_dt.predict(X_test)\n",
    "print(accuracy_score(y_test,best_model))\n",
    "'''pred = grid_dt.predict(X_test) 이것만으로도 가능(간단)\n",
    "best_model = grid_dt.best_estimator_\n",
    "pred = best_model.predict(X_test)이 원래는 정석'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "186b951b-f5d4-45d6-ba8d-a9c464c65894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8840555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 300, max_depth = 9)#n_estimators 추가\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_dt = rfc.predict(X_test)\n",
    "print(accuracy_score(y_test, rfc_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ad2a8db-f005-477f-a1c8-1869e91dcca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.63743\n",
      "[1]\tvalidation_0-logloss:0.59613\n",
      "[2]\tvalidation_0-logloss:0.56152\n",
      "[3]\tvalidation_0-logloss:0.53239\n",
      "[4]\tvalidation_0-logloss:0.50739\n",
      "[5]\tvalidation_0-logloss:0.48590\n",
      "[6]\tvalidation_0-logloss:0.46712\n",
      "[7]\tvalidation_0-logloss:0.45056\n",
      "[8]\tvalidation_0-logloss:0.43604\n",
      "[9]\tvalidation_0-logloss:0.42309\n",
      "[10]\tvalidation_0-logloss:0.41107\n",
      "[11]\tvalidation_0-logloss:0.40026\n",
      "[12]\tvalidation_0-logloss:0.39045\n",
      "[13]\tvalidation_0-logloss:0.38151\n",
      "[14]\tvalidation_0-logloss:0.37346\n",
      "[15]\tvalidation_0-logloss:0.36627\n",
      "[16]\tvalidation_0-logloss:0.35958\n",
      "[17]\tvalidation_0-logloss:0.35350\n",
      "[18]\tvalidation_0-logloss:0.34806\n",
      "[19]\tvalidation_0-logloss:0.34306\n",
      "[20]\tvalidation_0-logloss:0.33845\n",
      "[21]\tvalidation_0-logloss:0.33421\n",
      "[22]\tvalidation_0-logloss:0.33029\n",
      "[23]\tvalidation_0-logloss:0.32664\n",
      "[24]\tvalidation_0-logloss:0.32324\n",
      "[25]\tvalidation_0-logloss:0.32014\n",
      "[26]\tvalidation_0-logloss:0.31723\n",
      "[27]\tvalidation_0-logloss:0.31455\n",
      "[28]\tvalidation_0-logloss:0.31201\n",
      "[29]\tvalidation_0-logloss:0.30966\n",
      "[30]\tvalidation_0-logloss:0.30750\n",
      "[31]\tvalidation_0-logloss:0.30553\n",
      "[32]\tvalidation_0-logloss:0.30362\n",
      "[33]\tvalidation_0-logloss:0.30183\n",
      "[34]\tvalidation_0-logloss:0.30015\n",
      "[35]\tvalidation_0-logloss:0.29856\n",
      "[36]\tvalidation_0-logloss:0.29712\n",
      "[37]\tvalidation_0-logloss:0.29574\n",
      "[38]\tvalidation_0-logloss:0.29439\n",
      "[39]\tvalidation_0-logloss:0.29318\n",
      "[40]\tvalidation_0-logloss:0.29208\n",
      "[41]\tvalidation_0-logloss:0.29100\n",
      "[42]\tvalidation_0-logloss:0.28998\n",
      "[43]\tvalidation_0-logloss:0.28906\n",
      "[44]\tvalidation_0-logloss:0.28815\n",
      "[45]\tvalidation_0-logloss:0.28737\n",
      "[46]\tvalidation_0-logloss:0.28655\n",
      "[47]\tvalidation_0-logloss:0.28580\n",
      "[48]\tvalidation_0-logloss:0.28509\n",
      "[49]\tvalidation_0-logloss:0.28442\n",
      "[50]\tvalidation_0-logloss:0.28376\n",
      "[51]\tvalidation_0-logloss:0.28321\n",
      "[52]\tvalidation_0-logloss:0.28264\n",
      "[53]\tvalidation_0-logloss:0.28213\n",
      "[54]\tvalidation_0-logloss:0.28165\n",
      "[55]\tvalidation_0-logloss:0.28115\n",
      "[56]\tvalidation_0-logloss:0.28067\n",
      "[57]\tvalidation_0-logloss:0.28025\n",
      "[58]\tvalidation_0-logloss:0.27987\n",
      "[59]\tvalidation_0-logloss:0.27952\n",
      "[60]\tvalidation_0-logloss:0.27915\n",
      "[61]\tvalidation_0-logloss:0.27881\n",
      "[62]\tvalidation_0-logloss:0.27850\n",
      "[63]\tvalidation_0-logloss:0.27819\n",
      "[64]\tvalidation_0-logloss:0.27788\n",
      "[65]\tvalidation_0-logloss:0.27762\n",
      "[66]\tvalidation_0-logloss:0.27736\n",
      "[67]\tvalidation_0-logloss:0.27710\n",
      "[68]\tvalidation_0-logloss:0.27686\n",
      "[69]\tvalidation_0-logloss:0.27662\n",
      "[70]\tvalidation_0-logloss:0.27640\n",
      "[71]\tvalidation_0-logloss:0.27616\n",
      "[72]\tvalidation_0-logloss:0.27594\n",
      "[73]\tvalidation_0-logloss:0.27573\n",
      "[74]\tvalidation_0-logloss:0.27557\n",
      "[75]\tvalidation_0-logloss:0.27540\n",
      "[76]\tvalidation_0-logloss:0.27524\n",
      "[77]\tvalidation_0-logloss:0.27509\n",
      "[78]\tvalidation_0-logloss:0.27494\n",
      "[79]\tvalidation_0-logloss:0.27480\n",
      "[80]\tvalidation_0-logloss:0.27465\n",
      "[81]\tvalidation_0-logloss:0.27452\n",
      "[82]\tvalidation_0-logloss:0.27438\n",
      "[83]\tvalidation_0-logloss:0.27423\n",
      "[84]\tvalidation_0-logloss:0.27409\n",
      "[85]\tvalidation_0-logloss:0.27398\n",
      "[86]\tvalidation_0-logloss:0.27387\n",
      "[87]\tvalidation_0-logloss:0.27378\n",
      "[88]\tvalidation_0-logloss:0.27366\n",
      "[89]\tvalidation_0-logloss:0.27354\n",
      "[90]\tvalidation_0-logloss:0.27346\n",
      "[91]\tvalidation_0-logloss:0.27337\n",
      "[92]\tvalidation_0-logloss:0.27327\n",
      "[93]\tvalidation_0-logloss:0.27318\n",
      "[94]\tvalidation_0-logloss:0.27310\n",
      "[95]\tvalidation_0-logloss:0.27302\n",
      "[96]\tvalidation_0-logloss:0.27294\n",
      "[97]\tvalidation_0-logloss:0.27287\n",
      "[98]\tvalidation_0-logloss:0.27278\n",
      "[99]\tvalidation_0-logloss:0.27271\n",
      "[100]\tvalidation_0-logloss:0.27263\n",
      "[101]\tvalidation_0-logloss:0.27257\n",
      "[102]\tvalidation_0-logloss:0.27249\n",
      "[103]\tvalidation_0-logloss:0.27245\n",
      "[104]\tvalidation_0-logloss:0.27239\n",
      "[105]\tvalidation_0-logloss:0.27232\n",
      "[106]\tvalidation_0-logloss:0.27227\n",
      "[107]\tvalidation_0-logloss:0.27223\n",
      "[108]\tvalidation_0-logloss:0.27217\n",
      "[109]\tvalidation_0-logloss:0.27213\n",
      "[110]\tvalidation_0-logloss:0.27209\n",
      "[111]\tvalidation_0-logloss:0.27203\n",
      "[112]\tvalidation_0-logloss:0.27199\n",
      "[113]\tvalidation_0-logloss:0.27195\n",
      "[114]\tvalidation_0-logloss:0.27186\n",
      "[115]\tvalidation_0-logloss:0.27184\n",
      "[116]\tvalidation_0-logloss:0.27181\n",
      "[117]\tvalidation_0-logloss:0.27176\n",
      "[118]\tvalidation_0-logloss:0.27172\n",
      "[119]\tvalidation_0-logloss:0.27168\n",
      "[120]\tvalidation_0-logloss:0.27164\n",
      "[121]\tvalidation_0-logloss:0.27160\n",
      "[122]\tvalidation_0-logloss:0.27156\n",
      "[123]\tvalidation_0-logloss:0.27152\n",
      "[124]\tvalidation_0-logloss:0.27149\n",
      "[125]\tvalidation_0-logloss:0.27146\n",
      "[126]\tvalidation_0-logloss:0.27143\n",
      "[127]\tvalidation_0-logloss:0.27139\n",
      "[128]\tvalidation_0-logloss:0.27137\n",
      "[129]\tvalidation_0-logloss:0.27129\n",
      "[130]\tvalidation_0-logloss:0.27127\n",
      "[131]\tvalidation_0-logloss:0.27125\n",
      "[132]\tvalidation_0-logloss:0.27123\n",
      "[133]\tvalidation_0-logloss:0.27121\n",
      "[134]\tvalidation_0-logloss:0.27119\n",
      "[135]\tvalidation_0-logloss:0.27117\n",
      "[136]\tvalidation_0-logloss:0.27115\n",
      "[137]\tvalidation_0-logloss:0.27112\n",
      "[138]\tvalidation_0-logloss:0.27109\n",
      "[139]\tvalidation_0-logloss:0.27108\n",
      "[140]\tvalidation_0-logloss:0.27105\n",
      "[141]\tvalidation_0-logloss:0.27102\n",
      "[142]\tvalidation_0-logloss:0.27100\n",
      "[143]\tvalidation_0-logloss:0.27099\n",
      "[144]\tvalidation_0-logloss:0.27098\n",
      "[145]\tvalidation_0-logloss:0.27094\n",
      "[146]\tvalidation_0-logloss:0.27093\n",
      "[147]\tvalidation_0-logloss:0.27092\n",
      "[148]\tvalidation_0-logloss:0.27092\n",
      "[149]\tvalidation_0-logloss:0.27082\n",
      "[150]\tvalidation_0-logloss:0.27080\n",
      "[151]\tvalidation_0-logloss:0.27079\n",
      "[152]\tvalidation_0-logloss:0.27078\n",
      "[153]\tvalidation_0-logloss:0.27076\n",
      "[154]\tvalidation_0-logloss:0.27074\n",
      "[155]\tvalidation_0-logloss:0.27068\n",
      "[156]\tvalidation_0-logloss:0.27067\n",
      "[157]\tvalidation_0-logloss:0.27066\n",
      "[158]\tvalidation_0-logloss:0.27066\n",
      "[159]\tvalidation_0-logloss:0.27065\n",
      "[160]\tvalidation_0-logloss:0.27064\n",
      "[161]\tvalidation_0-logloss:0.27063\n",
      "[162]\tvalidation_0-logloss:0.27061\n",
      "[163]\tvalidation_0-logloss:0.27060\n",
      "[164]\tvalidation_0-logloss:0.27059\n",
      "[165]\tvalidation_0-logloss:0.27056\n",
      "[166]\tvalidation_0-logloss:0.27056\n",
      "[167]\tvalidation_0-logloss:0.27055\n",
      "[168]\tvalidation_0-logloss:0.27054\n",
      "[169]\tvalidation_0-logloss:0.27053\n",
      "[170]\tvalidation_0-logloss:0.27052\n",
      "[171]\tvalidation_0-logloss:0.27051\n",
      "[172]\tvalidation_0-logloss:0.27049\n",
      "[173]\tvalidation_0-logloss:0.27048\n",
      "[174]\tvalidation_0-logloss:0.27041\n",
      "[175]\tvalidation_0-logloss:0.27039\n",
      "[176]\tvalidation_0-logloss:0.27039\n",
      "[177]\tvalidation_0-logloss:0.27038\n",
      "[178]\tvalidation_0-logloss:0.27039\n",
      "[179]\tvalidation_0-logloss:0.27036\n",
      "[180]\tvalidation_0-logloss:0.27035\n",
      "[181]\tvalidation_0-logloss:0.27034\n",
      "[182]\tvalidation_0-logloss:0.27033\n",
      "[183]\tvalidation_0-logloss:0.27032\n",
      "[184]\tvalidation_0-logloss:0.27030\n",
      "[185]\tvalidation_0-logloss:0.27030\n",
      "[186]\tvalidation_0-logloss:0.27029\n",
      "[187]\tvalidation_0-logloss:0.27029\n",
      "[188]\tvalidation_0-logloss:0.27028\n",
      "[189]\tvalidation_0-logloss:0.27027\n",
      "[190]\tvalidation_0-logloss:0.27027\n",
      "[191]\tvalidation_0-logloss:0.27022\n",
      "[192]\tvalidation_0-logloss:0.27016\n",
      "[193]\tvalidation_0-logloss:0.27015\n",
      "[194]\tvalidation_0-logloss:0.27013\n",
      "[195]\tvalidation_0-logloss:0.27009\n",
      "[196]\tvalidation_0-logloss:0.27008\n",
      "[197]\tvalidation_0-logloss:0.27003\n",
      "[198]\tvalidation_0-logloss:0.27003\n",
      "[199]\tvalidation_0-logloss:0.27003\n",
      "[200]\tvalidation_0-logloss:0.27002\n",
      "[201]\tvalidation_0-logloss:0.27002\n",
      "[202]\tvalidation_0-logloss:0.27000\n",
      "[203]\tvalidation_0-logloss:0.27000\n",
      "[204]\tvalidation_0-logloss:0.27000\n",
      "[205]\tvalidation_0-logloss:0.26998\n",
      "[206]\tvalidation_0-logloss:0.26996\n",
      "[207]\tvalidation_0-logloss:0.26990\n",
      "[208]\tvalidation_0-logloss:0.26988\n",
      "[209]\tvalidation_0-logloss:0.26987\n",
      "[210]\tvalidation_0-logloss:0.26986\n",
      "[211]\tvalidation_0-logloss:0.26986\n",
      "[212]\tvalidation_0-logloss:0.26981\n",
      "[213]\tvalidation_0-logloss:0.26981\n",
      "[214]\tvalidation_0-logloss:0.26980\n",
      "[215]\tvalidation_0-logloss:0.26980\n",
      "[216]\tvalidation_0-logloss:0.26980\n",
      "[217]\tvalidation_0-logloss:0.26980\n",
      "[218]\tvalidation_0-logloss:0.26976\n",
      "[219]\tvalidation_0-logloss:0.26975\n",
      "[220]\tvalidation_0-logloss:0.26974\n",
      "[221]\tvalidation_0-logloss:0.26972\n",
      "[222]\tvalidation_0-logloss:0.26972\n",
      "[223]\tvalidation_0-logloss:0.26971\n",
      "[224]\tvalidation_0-logloss:0.26971\n",
      "[225]\tvalidation_0-logloss:0.26970\n",
      "[226]\tvalidation_0-logloss:0.26966\n",
      "[227]\tvalidation_0-logloss:0.26966\n",
      "[228]\tvalidation_0-logloss:0.26965\n",
      "[229]\tvalidation_0-logloss:0.26962\n",
      "[230]\tvalidation_0-logloss:0.26962\n",
      "[231]\tvalidation_0-logloss:0.26961\n",
      "[232]\tvalidation_0-logloss:0.26962\n",
      "[233]\tvalidation_0-logloss:0.26959\n",
      "[234]\tvalidation_0-logloss:0.26958\n",
      "[235]\tvalidation_0-logloss:0.26956\n",
      "[236]\tvalidation_0-logloss:0.26955\n",
      "[237]\tvalidation_0-logloss:0.26955\n",
      "[238]\tvalidation_0-logloss:0.26954\n",
      "[239]\tvalidation_0-logloss:0.26954\n",
      "[240]\tvalidation_0-logloss:0.26952\n",
      "[241]\tvalidation_0-logloss:0.26948\n",
      "[242]\tvalidation_0-logloss:0.26946\n",
      "[243]\tvalidation_0-logloss:0.26941\n",
      "[244]\tvalidation_0-logloss:0.26938\n",
      "[245]\tvalidation_0-logloss:0.26938\n",
      "[246]\tvalidation_0-logloss:0.26937\n",
      "[247]\tvalidation_0-logloss:0.26937\n",
      "[248]\tvalidation_0-logloss:0.26937\n",
      "[249]\tvalidation_0-logloss:0.26934\n",
      "[250]\tvalidation_0-logloss:0.26932\n",
      "[251]\tvalidation_0-logloss:0.26931\n",
      "[252]\tvalidation_0-logloss:0.26930\n",
      "[253]\tvalidation_0-logloss:0.26930\n",
      "[254]\tvalidation_0-logloss:0.26929\n",
      "[255]\tvalidation_0-logloss:0.26929\n",
      "[256]\tvalidation_0-logloss:0.26930\n",
      "[257]\tvalidation_0-logloss:0.26927\n",
      "[258]\tvalidation_0-logloss:0.26928\n",
      "[259]\tvalidation_0-logloss:0.26927\n",
      "[260]\tvalidation_0-logloss:0.26927\n",
      "[261]\tvalidation_0-logloss:0.26927\n",
      "[262]\tvalidation_0-logloss:0.26927\n",
      "[263]\tvalidation_0-logloss:0.26926\n",
      "[264]\tvalidation_0-logloss:0.26926\n",
      "[265]\tvalidation_0-logloss:0.26923\n",
      "[266]\tvalidation_0-logloss:0.26920\n",
      "[267]\tvalidation_0-logloss:0.26918\n",
      "[268]\tvalidation_0-logloss:0.26915\n",
      "[269]\tvalidation_0-logloss:0.26915\n",
      "[270]\tvalidation_0-logloss:0.26913\n",
      "[271]\tvalidation_0-logloss:0.26912\n",
      "[272]\tvalidation_0-logloss:0.26910\n",
      "[273]\tvalidation_0-logloss:0.26908\n",
      "[274]\tvalidation_0-logloss:0.26908\n",
      "[275]\tvalidation_0-logloss:0.26908\n",
      "[276]\tvalidation_0-logloss:0.26909\n",
      "[277]\tvalidation_0-logloss:0.26909\n",
      "[278]\tvalidation_0-logloss:0.26908\n",
      "[279]\tvalidation_0-logloss:0.26907\n",
      "[280]\tvalidation_0-logloss:0.26905\n",
      "[281]\tvalidation_0-logloss:0.26905\n",
      "[282]\tvalidation_0-logloss:0.26904\n",
      "[283]\tvalidation_0-logloss:0.26903\n",
      "[284]\tvalidation_0-logloss:0.26903\n",
      "[285]\tvalidation_0-logloss:0.26903\n",
      "[286]\tvalidation_0-logloss:0.26901\n",
      "[287]\tvalidation_0-logloss:0.26898\n",
      "[288]\tvalidation_0-logloss:0.26898\n",
      "[289]\tvalidation_0-logloss:0.26898\n",
      "[290]\tvalidation_0-logloss:0.26898\n",
      "[291]\tvalidation_0-logloss:0.26898\n",
      "[292]\tvalidation_0-logloss:0.26898\n",
      "[293]\tvalidation_0-logloss:0.26898\n",
      "[294]\tvalidation_0-logloss:0.26895\n",
      "[295]\tvalidation_0-logloss:0.26895\n",
      "[296]\tvalidation_0-logloss:0.26894\n",
      "[297]\tvalidation_0-logloss:0.26894\n",
      "[298]\tvalidation_0-logloss:0.26894\n",
      "[299]\tvalidation_0-logloss:0.26894\n",
      "[300]\tvalidation_0-logloss:0.26894\n",
      "[301]\tvalidation_0-logloss:0.26893\n",
      "[302]\tvalidation_0-logloss:0.26893\n",
      "[303]\tvalidation_0-logloss:0.26892\n",
      "[304]\tvalidation_0-logloss:0.26892\n",
      "[305]\tvalidation_0-logloss:0.26891\n",
      "[306]\tvalidation_0-logloss:0.26889\n",
      "[307]\tvalidation_0-logloss:0.26888\n",
      "[308]\tvalidation_0-logloss:0.26888\n",
      "[309]\tvalidation_0-logloss:0.26888\n",
      "[310]\tvalidation_0-logloss:0.26887\n",
      "[311]\tvalidation_0-logloss:0.26887\n",
      "[312]\tvalidation_0-logloss:0.26886\n",
      "[313]\tvalidation_0-logloss:0.26884\n",
      "[314]\tvalidation_0-logloss:0.26883\n",
      "[315]\tvalidation_0-logloss:0.26882\n",
      "[316]\tvalidation_0-logloss:0.26881\n",
      "[317]\tvalidation_0-logloss:0.26880\n",
      "[318]\tvalidation_0-logloss:0.26878\n",
      "[319]\tvalidation_0-logloss:0.26878\n",
      "[320]\tvalidation_0-logloss:0.26878\n",
      "[321]\tvalidation_0-logloss:0.26878\n",
      "[322]\tvalidation_0-logloss:0.26878\n",
      "[323]\tvalidation_0-logloss:0.26878\n",
      "[324]\tvalidation_0-logloss:0.26878\n",
      "[325]\tvalidation_0-logloss:0.26877\n",
      "[326]\tvalidation_0-logloss:0.26877\n",
      "[327]\tvalidation_0-logloss:0.26876\n",
      "[328]\tvalidation_0-logloss:0.26876\n",
      "[329]\tvalidation_0-logloss:0.26876\n",
      "[330]\tvalidation_0-logloss:0.26876\n",
      "[331]\tvalidation_0-logloss:0.26874\n",
      "[332]\tvalidation_0-logloss:0.26874\n",
      "[333]\tvalidation_0-logloss:0.26874\n",
      "[334]\tvalidation_0-logloss:0.26873\n",
      "[335]\tvalidation_0-logloss:0.26871\n",
      "[336]\tvalidation_0-logloss:0.26871\n",
      "[337]\tvalidation_0-logloss:0.26870\n",
      "[338]\tvalidation_0-logloss:0.26870\n",
      "[339]\tvalidation_0-logloss:0.26869\n",
      "[340]\tvalidation_0-logloss:0.26869\n",
      "[341]\tvalidation_0-logloss:0.26868\n",
      "[342]\tvalidation_0-logloss:0.26867\n",
      "[343]\tvalidation_0-logloss:0.26866\n",
      "[344]\tvalidation_0-logloss:0.26866\n",
      "[345]\tvalidation_0-logloss:0.26866\n",
      "[346]\tvalidation_0-logloss:0.26865\n",
      "[347]\tvalidation_0-logloss:0.26863\n",
      "[348]\tvalidation_0-logloss:0.26864\n",
      "[349]\tvalidation_0-logloss:0.26864\n",
      "[350]\tvalidation_0-logloss:0.26864\n",
      "[351]\tvalidation_0-logloss:0.26864\n",
      "[352]\tvalidation_0-logloss:0.26863\n",
      "[353]\tvalidation_0-logloss:0.26862\n",
      "[354]\tvalidation_0-logloss:0.26860\n",
      "[355]\tvalidation_0-logloss:0.26860\n",
      "[356]\tvalidation_0-logloss:0.26859\n",
      "[357]\tvalidation_0-logloss:0.26858\n",
      "[358]\tvalidation_0-logloss:0.26859\n",
      "[359]\tvalidation_0-logloss:0.26857\n",
      "[360]\tvalidation_0-logloss:0.26855\n",
      "[361]\tvalidation_0-logloss:0.26854\n",
      "[362]\tvalidation_0-logloss:0.26854\n",
      "[363]\tvalidation_0-logloss:0.26853\n",
      "[364]\tvalidation_0-logloss:0.26852\n",
      "[365]\tvalidation_0-logloss:0.26852\n",
      "[366]\tvalidation_0-logloss:0.26852\n",
      "[367]\tvalidation_0-logloss:0.26851\n",
      "[368]\tvalidation_0-logloss:0.26851\n",
      "[369]\tvalidation_0-logloss:0.26850\n",
      "[370]\tvalidation_0-logloss:0.26850\n",
      "[371]\tvalidation_0-logloss:0.26849\n",
      "[372]\tvalidation_0-logloss:0.26849\n",
      "[373]\tvalidation_0-logloss:0.26848\n",
      "[374]\tvalidation_0-logloss:0.26848\n",
      "[375]\tvalidation_0-logloss:0.26848\n",
      "[376]\tvalidation_0-logloss:0.26848\n",
      "[377]\tvalidation_0-logloss:0.26848\n",
      "[378]\tvalidation_0-logloss:0.26848\n",
      "[379]\tvalidation_0-logloss:0.26848\n",
      "[380]\tvalidation_0-logloss:0.26847\n",
      "[381]\tvalidation_0-logloss:0.26847\n",
      "[382]\tvalidation_0-logloss:0.26847\n",
      "[383]\tvalidation_0-logloss:0.26847\n",
      "[384]\tvalidation_0-logloss:0.26847\n",
      "[385]\tvalidation_0-logloss:0.26847\n",
      "[386]\tvalidation_0-logloss:0.26847\n",
      "[387]\tvalidation_0-logloss:0.26847\n",
      "[388]\tvalidation_0-logloss:0.26847\n",
      "[389]\tvalidation_0-logloss:0.26846\n",
      "[390]\tvalidation_0-logloss:0.26846\n",
      "[391]\tvalidation_0-logloss:0.26846\n",
      "[392]\tvalidation_0-logloss:0.26846\n",
      "[393]\tvalidation_0-logloss:0.26845\n",
      "[394]\tvalidation_0-logloss:0.26845\n",
      "[395]\tvalidation_0-logloss:0.26845\n",
      "[396]\tvalidation_0-logloss:0.26845\n",
      "[397]\tvalidation_0-logloss:0.26845\n",
      "[398]\tvalidation_0-logloss:0.26845\n",
      "[399]\tvalidation_0-logloss:0.26845\n",
      "[400]\tvalidation_0-logloss:0.26845\n",
      "[401]\tvalidation_0-logloss:0.26844\n",
      "[402]\tvalidation_0-logloss:0.26844\n",
      "[403]\tvalidation_0-logloss:0.26845\n",
      "[404]\tvalidation_0-logloss:0.26844\n",
      "[405]\tvalidation_0-logloss:0.26844\n",
      "[406]\tvalidation_0-logloss:0.26842\n",
      "[407]\tvalidation_0-logloss:0.26842\n",
      "[408]\tvalidation_0-logloss:0.26842\n",
      "[409]\tvalidation_0-logloss:0.26842\n",
      "[410]\tvalidation_0-logloss:0.26841\n",
      "[411]\tvalidation_0-logloss:0.26841\n",
      "[412]\tvalidation_0-logloss:0.26841\n",
      "[413]\tvalidation_0-logloss:0.26840\n",
      "[414]\tvalidation_0-logloss:0.26840\n",
      "[415]\tvalidation_0-logloss:0.26839\n",
      "[416]\tvalidation_0-logloss:0.26839\n",
      "[417]\tvalidation_0-logloss:0.26839\n",
      "[418]\tvalidation_0-logloss:0.26839\n",
      "[419]\tvalidation_0-logloss:0.26838\n",
      "[420]\tvalidation_0-logloss:0.26838\n",
      "[421]\tvalidation_0-logloss:0.26838\n",
      "[422]\tvalidation_0-logloss:0.26838\n",
      "[423]\tvalidation_0-logloss:0.26837\n",
      "[424]\tvalidation_0-logloss:0.26836\n",
      "[425]\tvalidation_0-logloss:0.26835\n",
      "[426]\tvalidation_0-logloss:0.26835\n",
      "[427]\tvalidation_0-logloss:0.26835\n",
      "[428]\tvalidation_0-logloss:0.26835\n",
      "[429]\tvalidation_0-logloss:0.26835\n",
      "[430]\tvalidation_0-logloss:0.26834\n",
      "[431]\tvalidation_0-logloss:0.26835\n",
      "[432]\tvalidation_0-logloss:0.26834\n",
      "[433]\tvalidation_0-logloss:0.26835\n",
      "[434]\tvalidation_0-logloss:0.26835\n",
      "[435]\tvalidation_0-logloss:0.26835\n",
      "[436]\tvalidation_0-logloss:0.26835\n",
      "[437]\tvalidation_0-logloss:0.26835\n",
      "[438]\tvalidation_0-logloss:0.26835\n",
      "[439]\tvalidation_0-logloss:0.26834\n",
      "[440]\tvalidation_0-logloss:0.26833\n",
      "[441]\tvalidation_0-logloss:0.26832\n",
      "[442]\tvalidation_0-logloss:0.26832\n",
      "[443]\tvalidation_0-logloss:0.26832\n",
      "[444]\tvalidation_0-logloss:0.26831\n",
      "[445]\tvalidation_0-logloss:0.26832\n",
      "[446]\tvalidation_0-logloss:0.26831\n",
      "[447]\tvalidation_0-logloss:0.26831\n",
      "[448]\tvalidation_0-logloss:0.26830\n",
      "[449]\tvalidation_0-logloss:0.26830\n",
      "[450]\tvalidation_0-logloss:0.26829\n",
      "[451]\tvalidation_0-logloss:0.26829\n",
      "[452]\tvalidation_0-logloss:0.26829\n",
      "[453]\tvalidation_0-logloss:0.26830\n",
      "[454]\tvalidation_0-logloss:0.26830\n",
      "[455]\tvalidation_0-logloss:0.26830\n",
      "[456]\tvalidation_0-logloss:0.26830\n",
      "[457]\tvalidation_0-logloss:0.26830\n",
      "[458]\tvalidation_0-logloss:0.26829\n",
      "[459]\tvalidation_0-logloss:0.26829\n",
      "[460]\tvalidation_0-logloss:0.26829\n",
      "[461]\tvalidation_0-logloss:0.26828\n",
      "[462]\tvalidation_0-logloss:0.26827\n",
      "[463]\tvalidation_0-logloss:0.26827\n",
      "[464]\tvalidation_0-logloss:0.26827\n",
      "[465]\tvalidation_0-logloss:0.26827\n",
      "[466]\tvalidation_0-logloss:0.26827\n",
      "[467]\tvalidation_0-logloss:0.26827\n",
      "[468]\tvalidation_0-logloss:0.26827\n",
      "[469]\tvalidation_0-logloss:0.26827\n",
      "[470]\tvalidation_0-logloss:0.26827\n",
      "[471]\tvalidation_0-logloss:0.26827\n",
      "[472]\tvalidation_0-logloss:0.26826\n",
      "[473]\tvalidation_0-logloss:0.26826\n",
      "[474]\tvalidation_0-logloss:0.26826\n",
      "[475]\tvalidation_0-logloss:0.26826\n",
      "[476]\tvalidation_0-logloss:0.26826\n",
      "[477]\tvalidation_0-logloss:0.26826\n",
      "[478]\tvalidation_0-logloss:0.26825\n",
      "[479]\tvalidation_0-logloss:0.26825\n",
      "[480]\tvalidation_0-logloss:0.26824\n",
      "[481]\tvalidation_0-logloss:0.26824\n",
      "[482]\tvalidation_0-logloss:0.26824\n",
      "[483]\tvalidation_0-logloss:0.26824\n",
      "[484]\tvalidation_0-logloss:0.26824\n",
      "[485]\tvalidation_0-logloss:0.26825\n",
      "[486]\tvalidation_0-logloss:0.26824\n",
      "[487]\tvalidation_0-logloss:0.26824\n",
      "[488]\tvalidation_0-logloss:0.26824\n",
      "[489]\tvalidation_0-logloss:0.26824\n",
      "[490]\tvalidation_0-logloss:0.26823\n",
      "[491]\tvalidation_0-logloss:0.26823\n",
      "[492]\tvalidation_0-logloss:0.26823\n",
      "[493]\tvalidation_0-logloss:0.26824\n",
      "[494]\tvalidation_0-logloss:0.26823\n",
      "[495]\tvalidation_0-logloss:0.26823\n",
      "[496]\tvalidation_0-logloss:0.26823\n",
      "[497]\tvalidation_0-logloss:0.26824\n",
      "[498]\tvalidation_0-logloss:0.26824\n",
      "[499]\tvalidation_0-logloss:0.26824\n",
      "0.8885952380952381\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier#XGBoostClassifier아님\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "xgb = XGBClassifier(n_estimators = 500, learning_rate = 0.1, max_depth = 3, eval_metric = 'logloss')#early_stopping_rounds = 100\n",
    "'''원래는 early_stopping_rounds를 썼지만 voting을 위해 제거'''\n",
    "xgb.fit(X_train,y_train, eval_set=[(X_val, y_val)])#early_stopping_rounds, eval_metric은 위로\n",
    "xgb_predict = xgb.predict(X_test)\n",
    "print(accuracy_score(y_test,xgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea6ae0c7-53d3-405e-b9ab-bb6f0b5bd681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 180985, number of negative: 222215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 403200, number of used features: 14\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448872 -> initscore=-0.205231\n",
      "[LightGBM] [Info] Start training from score -0.205231\n",
      "[10]\tvalid_0's binary_logloss: 0.390536\n",
      "[20]\tvalid_0's binary_logloss: 0.314978\n",
      "[30]\tvalid_0's binary_logloss: 0.288868\n",
      "[40]\tvalid_0's binary_logloss: 0.278881\n",
      "[50]\tvalid_0's binary_logloss: 0.274764\n",
      "[60]\tvalid_0's binary_logloss: 0.272815\n",
      "[70]\tvalid_0's binary_logloss: 0.271811\n",
      "[80]\tvalid_0's binary_logloss: 0.270944\n",
      "[90]\tvalid_0's binary_logloss: 0.27047\n",
      "[100]\tvalid_0's binary_logloss: 0.270234\n",
      "[110]\tvalid_0's binary_logloss: 0.270091\n",
      "[120]\tvalid_0's binary_logloss: 0.269899\n",
      "[130]\tvalid_0's binary_logloss: 0.269696\n",
      "[140]\tvalid_0's binary_logloss: 0.2695\n",
      "[150]\tvalid_0's binary_logloss: 0.269494\n",
      "[160]\tvalid_0's binary_logloss: 0.2695\n",
      "[170]\tvalid_0's binary_logloss: 0.269486\n",
      "[180]\tvalid_0's binary_logloss: 0.269489\n",
      "[190]\tvalid_0's binary_logloss: 0.26945\n",
      "[200]\tvalid_0's binary_logloss: 0.269479\n",
      "[210]\tvalid_0's binary_logloss: 0.269455\n",
      "[220]\tvalid_0's binary_logloss: 0.269456\n",
      "[230]\tvalid_0's binary_logloss: 0.269489\n",
      "[240]\tvalid_0's binary_logloss: 0.269454\n",
      "[250]\tvalid_0's binary_logloss: 0.269492\n",
      "[260]\tvalid_0's binary_logloss: 0.269495\n",
      "[270]\tvalid_0's binary_logloss: 0.269467\n",
      "[280]\tvalid_0's binary_logloss: 0.269518\n",
      "[290]\tvalid_0's binary_logloss: 0.269527\n",
      "[300]\tvalid_0's binary_logloss: 0.269531\n",
      "[310]\tvalid_0's binary_logloss: 0.269565\n",
      "[320]\tvalid_0's binary_logloss: 0.269568\n",
      "[330]\tvalid_0's binary_logloss: 0.269568\n",
      "[340]\tvalid_0's binary_logloss: 0.269581\n",
      "[350]\tvalid_0's binary_logloss: 0.269614\n",
      "[360]\tvalid_0's binary_logloss: 0.269644\n",
      "[370]\tvalid_0's binary_logloss: 0.269657\n",
      "[380]\tvalid_0's binary_logloss: 0.26966\n",
      "[390]\tvalid_0's binary_logloss: 0.269676\n",
      "[400]\tvalid_0's binary_logloss: 0.269679\n",
      "[410]\tvalid_0's binary_logloss: 0.269702\n",
      "[420]\tvalid_0's binary_logloss: 0.269699\n",
      "[430]\tvalid_0's binary_logloss: 0.26972\n",
      "[440]\tvalid_0's binary_logloss: 0.26973\n",
      "[450]\tvalid_0's binary_logloss: 0.269796\n",
      "[460]\tvalid_0's binary_logloss: 0.269813\n",
      "[470]\tvalid_0's binary_logloss: 0.269835\n",
      "[480]\tvalid_0's binary_logloss: 0.269849\n",
      "[490]\tvalid_0's binary_logloss: 0.269877\n",
      "[500]\tvalid_0's binary_logloss: 0.269949\n",
      "0.8881507936507936\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping, log_evaluation#버전이 바뀌면서 미리 지정해 줘야함\n",
    "lgbm = LGBMClassifier(n_estimators = 500, learning_rate = 0.1)\n",
    "lgbm.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'logloss', callbacks = [log_evaluation(10)])# callbacks[early_stopping(stopping_rounds = 100)]\n",
    "'''early_stopping_rounds와 log_evaluation은 callback안에 작성, eval_set안은 []안에 한번 더 ()로 묶어줘야 함, 원래는 early_stopping을 썼지만 voting을 위해 제거'''\n",
    "lgb_predict = lgbm.predict(X_test)\n",
    "print(accuracy_score(y_test,lgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "081c1206-c921-4259-a999-240c47f998c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 180985, number of negative: 222215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 403200, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448872 -> initscore=-0.205231\n",
      "[LightGBM] [Info] Start training from score -0.205231\n",
      "0.888515873015873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "vt = VotingClassifier(estimators = [('xgboost', xgb),('lightgbm',lgbm)], voting = 'soft')#estimators도 []안에 ()로 각각 묶음\n",
    "vt.fit(X_train, y_train)\n",
    "vt_predict = vt.predict(X_test)\n",
    "print(accuracy_score(y_test, vt_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dfb9200-ec53-479c-91af-9d4f71f90584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120657, number of negative: 148143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 671\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448873 -> initscore=-0.205226\n",
      "[LightGBM] [Info] Start training from score -0.205226\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 120656, number of negative: 148144\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 672\n",
      "[LightGBM] [Info] Number of data points in the train set: 268800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448869 -> initscore=-0.205241\n",
      "[LightGBM] [Info] Start training from score -0.205241\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 180985, number of negative: 222215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 403200, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448872 -> initscore=-0.205231\n",
      "[LightGBM] [Info] Start training from score -0.205231\n",
      "0.8878333333333334\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "params = {'num_leaves':[32,64], 'max_depth':[10,20], 'n_estimators':[100,200,300,400] }#[]로 감싸야 함\n",
    "lgbm_grid = GridSearchCV(lgbm, param_grid = params,cv = 3)\n",
    "lgbm_grid.fit(X_train, y_train)\n",
    "lgbm_grid_predict = lgbm_grid.predict(X_test)\n",
    "print(accuracy_score(y_test, lgbm_grid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33e73597-53c2-4d88-b570-8775b2cb4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "'''final_pred = lgbm_grid.predict(test)처럼 통째로 넣으면 안됨'''\n",
    "submission_id = test['id']#id를 미리 따로 빼서 저장 >> inplace = True를 안하면 굳이 임\n",
    "test_input = test.drop('id', axis = 1)\n",
    "'''from sklearn.preprocessing import LabelEncoder >> test에는 heart disease컬럼이 없으므로 인코딩 필요 x'''\n",
    "final_pred = lgbm_grid.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3110832-9e1a-46b3-bc2f-7ef3adceed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  Heart Disease\n",
      "0  630000              1\n",
      "1  630001              0\n",
      "2  630002              1\n",
      "3  630003              0\n",
      "4  630004              0\n"
     ]
    }
   ],
   "source": [
    "# 딕셔너리 형태로 묶어서 DataFrame을 만듭니다.\n",
    "submission = pd.DataFrame({\n",
    "    'id': submission_id,          # 원본 데이터에서 id만 쏙 가져오기\n",
    "    'Heart Disease': final_pred      # 방금 만든 예측값 변수 넣기, 같은 모델로 inverse_transform()인줄 알았으나 숫자형으로 들어감\n",
    "})\n",
    "\n",
    "# 확인해보기 (이미지랑 똑같이 생겼는지)\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d15e2a2-ba0f-43d2-ab86-f42b40c1b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)# .to_csv로 csv파일로 변경, index = false는 필수임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fbe37d7-4963-41e8-bcbd-1fca5d19a49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Heart Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>630002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  Heart Disease\n",
       "0  630000              1\n",
       "1  630001              0\n",
       "2  630002              1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_csv = pd.read_csv('submission.csv')\n",
    "submission_csv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a37e9b59-6cb2-4825-80f8-b77e1ef562ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20260216_142742\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.5\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1+cpu\n",
      "CUDA Version:       CUDA is not available\n",
      "Memory Avail:       6.00 GB / 15.73 GB (38.2%)\n",
      "Disk Space Avail:   286.70 GB / 459.68 GB (62.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"C:\\Users\\hsk20\\predicting_heart_disease(26_2)\\AutogluonModels\\ag-20260216_142742\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 150s\n",
      "AutoGluon will save models to \"C:\\Users\\hsk20\\predicting_heart_disease(26_2)\\AutogluonModels\\ag-20260216_142742\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    560000\n",
      "Train Data Columns: 14\n",
      "Label Column:       Heart Disease\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = Presence, class 0 = Absence\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (Presence) vs negative (Absence) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5938.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.81 MB (1.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  1 | ['ST depression']\n",
      "\t\t('int', [])   : 13 | ['id', 'Age', 'Sex', 'Chest pain type', 'BP', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['ST depression']\n",
      "\t\t('int', [])       : 10 | ['id', 'Age', 'Chest pain type', 'BP', 'Cholesterol', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['Sex', 'FBS over 120', 'Exercise angina']\n",
      "\t3.4s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.60 MB (0.8% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 96.91s of the 145.34s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 162. Best iteration is:\n",
      "\t[157]\tvalid_set's binary_error: 0.123643\n",
      "\tRan out of time, early stopping on iteration 173. Best iteration is:\n",
      "\t[173]\tvalid_set's binary_error: 0.1242\n",
      "\tRan out of time, early stopping on iteration 179. Best iteration is:\n",
      "\t[176]\tvalid_set's binary_error: 0.125143\n",
      "\tRan out of time, early stopping on iteration 172. Best iteration is:\n",
      "\t[136]\tvalid_set's binary_error: 0.1254\n",
      "\tRan out of time, early stopping on iteration 193. Best iteration is:\n",
      "\t[190]\tvalid_set's binary_error: 0.123471\n",
      "\tRan out of time, early stopping on iteration 208. Best iteration is:\n",
      "\t[176]\tvalid_set's binary_error: 0.1234\n",
      "\t0.8758\t = Validation score   (accuracy)\n",
      "\t85.64s\t = Training   runtime\n",
      "\t3.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6.35s of the 54.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_error: 0.448343\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 3.05s of the 51.48s of remaining time.\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 2.608 GB out of 6.021 GB available memory (43.313%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.13 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=2.6/6.0 GB\n",
      "\tWarning: Model is expected to require 267.6s to train, which exceeds the maximum time limit of 2.5s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 145.41s of the 46.26s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/6.0 GB\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.8758\t = Validation score   (accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 45.92s of the 45.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 59. Best iteration is:\n",
      "\t[56]\tvalid_set's binary_error: 0.124\n",
      "\tRan out of time, early stopping on iteration 59. Best iteration is:\n",
      "\t[57]\tvalid_set's binary_error: 0.124443\n",
      "\tRan out of time, early stopping on iteration 62. Best iteration is:\n",
      "\t[62]\tvalid_set's binary_error: 0.124614\n",
      "\tRan out of time, early stopping on iteration 63. Best iteration is:\n",
      "\t[62]\tvalid_set's binary_error: 0.126171\n",
      "\tRan out of time, early stopping on iteration 72. Best iteration is:\n",
      "\t[72]\tvalid_set's binary_error: 0.125443\n",
      "\tRan out of time, early stopping on iteration 61. Best iteration is:\n",
      "\t[58]\tvalid_set's binary_error: 0.124529\n",
      "\tRan out of time, early stopping on iteration 80. Best iteration is:\n",
      "\t[79]\tvalid_set's binary_error: 0.126557\n",
      "\tRan out of time, early stopping on iteration 99. Best iteration is:\n",
      "\t[96]\tvalid_set's binary_error: 0.124514\n",
      "\t0.875\t = Validation score   (accuracy)\n",
      "\t42.05s\t = Training   runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 145.41s of the 0.36s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/6.0 GB\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.8758\t = Validation score   (accuracy)\n",
      "\t2.47s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 152.16s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 23210.2 rows/s (70000 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric accuracy | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.8758\n",
      "\tBest Threshold: 0.500\t| val: 0.8758\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\hsk20\\predicting_heart_disease(26_2)\\AutogluonModels\\ag-20260216_142742\\ds_sub_fit\\sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1       0.878414   0.875811    accuracy        3.448416       3.004719   85.635335                 3.448416                3.004719          85.635335            1       True          1\n",
      "1  WeightedEnsemble_L3       0.878414   0.875811    accuracy        3.494509       3.083989   88.108332                 0.046093                0.079269           2.472997            3       True          4\n",
      "2  WeightedEnsemble_L2       0.878414   0.875811    accuracy        3.506604       3.094257   85.698184                 0.058188                0.089538           0.062849            2       True          2\n",
      "3    LightGBMXT_BAG_L2       0.878157   0.874966    accuracy        5.130478       4.193062  127.687238                 1.682062                1.188342          42.051903            2       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t169s\t = DyStack   runtime |\t431s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 431s\n",
      "AutoGluon will save models to \"C:\\Users\\hsk20\\predicting_heart_disease(26_2)\\AutogluonModels\\ag-20260216_142742\"\n",
      "Train Data Rows:    630000\n",
      "Train Data Columns: 14\n",
      "Label Column:       Heart Disease\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = Presence, class 0 = Absence\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (Presence) vs negative (Absence) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6132.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.29 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :  1 | ['ST depression']\n",
      "\t\t('int', [])   : 13 | ['id', 'Age', 'Sex', 'Chest pain type', 'BP', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  1 | ['ST depression']\n",
      "\t\t('int', [])       : 10 | ['id', 'Age', 'Chest pain type', 'BP', 'Cholesterol', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['Sex', 'FBS over 120', 'Exercise angina']\n",
      "\t3.7s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.67 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 284.01s of the 426.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 633. Best iteration is:\n",
      "\t[617]\tvalid_set's binary_error: 0.123543\n",
      "\tRan out of time, early stopping on iteration 644. Best iteration is:\n",
      "\t[532]\tvalid_set's binary_error: 0.121689\n",
      "\t0.8769\t = Validation score   (accuracy)\n",
      "\t195.4s\t = Training   runtime\n",
      "\t7.29s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 79.06s of the 221.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 115. Best iteration is:\n",
      "\t[100]\tvalid_set's binary_error: 0.112673\n",
      "\tRan out of time, early stopping on iteration 116. Best iteration is:\n",
      "\t[116]\tvalid_set's binary_error: 0.112559\n",
      "\tRan out of time, early stopping on iteration 123. Best iteration is:\n",
      "\t[122]\tvalid_set's binary_error: 0.112013\n",
      "\tRan out of time, early stopping on iteration 122. Best iteration is:\n",
      "\t[99]\tvalid_set's binary_error: 0.114641\n",
      "\tRan out of time, early stopping on iteration 136. Best iteration is:\n",
      "\t[135]\tvalid_set's binary_error: 0.112102\n",
      "\tRan out of time, early stopping on iteration 146. Best iteration is:\n",
      "\t[140]\tvalid_set's binary_error: 0.112686\n",
      "\tRan out of time, early stopping on iteration 159. Best iteration is:\n",
      "\t[132]\tvalid_set's binary_error: 0.112952\n",
      "\tRan out of time, early stopping on iteration 180. Best iteration is:\n",
      "\t[178]\tvalid_set's binary_error: 0.111467\n",
      "\t0.8874\t = Validation score   (accuracy)\n",
      "\t72.92s\t = Training   runtime\n",
      "\t2.49s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1.84s of the 143.92s of remaining time.\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 2.934 GB out of 6.130 GB available memory (47.858%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.25 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=2.9/6.1 GB\n",
      "\tWarning: Model is expected to require 265.5s to train, which exceeds the maximum time limit of 1.2s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 138.65s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/6.1 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.8874\t = Validation score   (accuracy)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 135.76s of the 135.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 242. Best iteration is:\n",
      "\t[242]\tvalid_set's binary_error: 0.112394\n",
      "\tRan out of time, early stopping on iteration 265. Best iteration is:\n",
      "\t[221]\tvalid_set's binary_error: 0.112178\n",
      "\tRan out of time, early stopping on iteration 308. Best iteration is:\n",
      "\t[238]\tvalid_set's binary_error: 0.111543\n",
      "\t0.8869\t = Validation score   (accuracy)\n",
      "\t114.49s\t = Training   runtime\n",
      "\t3.68s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 15.40s of the 15.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_error: 0.448343\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 11.44s of the 11.32s of remaining time.\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 2.934 GB out of 6.072 GB available memory (48.318%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.26 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=2.9/6.1 GB\n",
      "\tWarning: Model is expected to require 450.2s to train, which exceeds the maximum time limit of 10.5s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 3.18s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/6.0 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.8874\t = Validation score   (accuracy)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 431.96s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 31543.3 rows/s (78750 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric accuracy | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.8874\n",
      "\tBest Threshold: 0.500\t| val: 0.8874\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\hsk20\\predicting_heart_disease(26_2)\\AutogluonModels\\ag-20260216_142742\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "# 1. 학습 (train 데이터 통째로 넣고, 정답 컬럼 이름만 알려주면 끝) >> 무조건 원본 데이터\n",
    "# time_limit : 몇 초 동안 학습할지 (길수록 성능 좋아짐, 예: 600초)\n",
    "predictor = TabularPredictor(label='Heart Disease', eval_metric='accuracy').fit(\n",
    "    train_data=train, \n",
    "    time_limit=600, \n",
    "    presets='best_quality')\n",
    "\n",
    "# 2. 리더보드 확인 (어떤 모델이 1등 했는지 보여줌)\n",
    "predictor.leaderboard(train)\n",
    "\n",
    "# 3. 예측 (test 데이터 넣기)\n",
    "pred_y = predictor.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9243786b-4b09-4b3f-bf66-ba92b23db8a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Heart Disease'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Heart Disease'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# test 데이터 안에 정답 컬럼이 포함되어 있는 경우\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m최종 정확도:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHeart Disease\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, pred_y))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Heart Disease'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "261ff4bf-bc7c-45ff-8127-8313e78fb675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Chest pain type</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FBS over 120</th>\n",
       "      <th>EKG results</th>\n",
       "      <th>Max HR</th>\n",
       "      <th>Exercise angina</th>\n",
       "      <th>ST depression</th>\n",
       "      <th>Slope of ST</th>\n",
       "      <th>Number of vessels fluro</th>\n",
       "      <th>Thallium</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
       "0   0   58    1                4  152          239             0            0   \n",
       "1   1   52    1                1  125          325             0            2   \n",
       "2   2   56    0                2  160          188             0            2   \n",
       "3   3   44    0                3  134          229             0            2   \n",
       "4   4   58    1                4  140          234             0            2   \n",
       "\n",
       "   Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
       "0     158                1            3.6            2   \n",
       "1     171                0            0.0            1   \n",
       "2     151                0            0.0            1   \n",
       "3     150                0            1.0            2   \n",
       "4     125                1            3.8            2   \n",
       "\n",
       "   Number of vessels fluro  Thallium  target  \n",
       "0                        2         7       1  \n",
       "1                        0         3       0  \n",
       "2                        0         3       0  \n",
       "3                        0         3       0  \n",
       "4                        3         3       1  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EDA\n",
    "train_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec69e19-e6dc-4d2b-969d-9c021f6bbdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
